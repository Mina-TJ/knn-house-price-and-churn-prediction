}
mean_acc[ki] <- mean(boot_acc, na.rm = TRUE)
}
# 3. Evaluate with that optimal k on that iteration's test set
optimal_k_cls[i] <- k_grid[which.max(replace(mean_acc, is.na(mean_acc), -Inf))]
if (i == n_iter) last_mean_acc <- mean_acc
final_model <- knn3(Churn ~ ., data = train, k = optimal_k_cls[i])
test_preds  <- predict(final_model, test, type = "class")
# 4. Store optimal_k and all metrics for each iteration
cm          <- confusionMatrix(test_preds, test$Churn, positive = "Yes")
acc_vec[i]  <- cm$overall["Accuracy"]
prec_vec[i] <- cm$byClass["Precision"]
rec_vec[i]  <- cm$byClass["Recall"]
f1_vec[i]   <- cm$byClass["F1"]
# Save last iteration for Q5
if (i == n_iter) rep_cm <- cm
}
# 5. Report mean and SD for all metrics
metrics_df <- data.frame(
Metric = c("Accuracy", "Precision", "Recall", "F1"),
Mean   = round(c(mean(acc_vec), mean(prec_vec), mean(rec_vec), mean(f1_vec)), 4),
SD     = round(c(sd(acc_vec),   sd(prec_vec),   sd(rec_vec),   sd(f1_vec)),   4)
)
print(metrics_df)
cat("\nOptimal k distribution:\n"); print(table(optimal_k_cls))
# Q3: Mean accuracy vs k
plot(k_grid, last_mean_acc, type = "b",
xlab = "k",
ylab = "Mean Accuracy",
main = "Mean Accuracy vs k (Last Iteration)")
# Q4: Histograms for all metrics
par(mfrow = c(2, 2))
hist(acc_vec,  main = "Accuracy",  xlab = "Accuracy",  col = "lightblue", border = "white")
hist(prec_vec, main = "Precision", xlab = "Precision", col = "lightblue", border = "white")
hist(rec_vec,  main = "Recall",    xlab = "Recall",    col = "lightblue", border = "white")
hist(f1_vec,   main = "F1-Score",  xlab = "F1",        col = "lightblue", border = "white")
par(mfrow = c(1, 1))
# Q5: Confusion matrix from representative split
print(rep_cm)
# YOUR CODE HERE: Display confusion matrix from one representative split
print(rep_cm)
# YOUR CODE HERE (optional)
# 1. Use knn3 with type = "prob" to get predicted probabilities
# 2. Try multiple thresholds
# 3. Compute metrics for each threshold
# 4. Create a table or plot
# YOUR CODE HERE (optional)
# 1. Use knn3 with type = "prob" to get predicted probabilities
# 2. Try multiple thresholds
# 3. Compute metrics for each threshold
# 4. Create a table or plot
# Use optimal k from last iteration and representative split
set.seed(123 + 20)
train_idx_rep <- createDataPartition(churn_data$Churn, p = 0.8, list = FALSE)
train_raw_rep <- churn_data[train_idx_rep, ]
test_raw_rep  <- churn_data[-train_idx_rep, ]
# dummyVars on predictors only
dummies_rep   <- dummyVars(~ ., data = train_raw_rep %>% select(-Churn))
# FIX: predict on predictors only (exclude Churn)
train_mat_rep <- predict(dummies_rep, train_raw_rep %>% select(-Churn))
test_mat_rep  <- predict(dummies_rep, test_raw_rep  %>% select(-Churn))
# Standardize: fit on train only, apply to both
pre_proc_rep  <- preProcess(train_mat_rep, method = c("center", "scale"))
train_mat_rep <- predict(pre_proc_rep, train_mat_rep)
test_mat_rep  <- predict(pre_proc_rep, test_mat_rep)
train_rep <- data.frame(train_mat_rep, Churn = factor(train_raw_rep$Churn, levels = c("No", "Yes")))
test_rep  <- data.frame(test_mat_rep,  Churn = factor(test_raw_rep$Churn,  levels = c("No", "Yes")))
# Fit with optimal k from last iteration
best_k_cls  <- optimal_k_cls[20]
bonus_model <- knn3(Churn ~ ., data = train_rep, k = best_k_cls)
# 1. Get predicted probabilities
probs <- predict(bonus_model, test_rep, type = "prob")
# 2. Try multiple thresholds
thresholds <- c(0.2, 0.3, 0.4, 0.5, 0.6)
threshold_results <- data.frame(
Threshold = thresholds,
Accuracy  = NA, Precision = NA, Recall = NA, F1 = NA
)
# 3. Compute metrics for each threshold
for (t in seq_along(thresholds)) {
pred_custom <- ifelse(probs[, "Yes"] >= thresholds[t], "Yes", "No")
pred_custom <- factor(pred_custom, levels = c("No", "Yes"))
cm_t <- confusionMatrix(pred_custom, test_rep$Churn, positive = "Yes")
threshold_results$Accuracy[t]  <- round(cm_t$overall["Accuracy"], 4)
threshold_results$Precision[t] <- round(cm_t$byClass["Precision"], 4)
threshold_results$Recall[t]    <- round(cm_t$byClass["Recall"], 4)
threshold_results$F1[t]        <- round(cm_t$byClass["F1"], 4)
}
print(threshold_results)
# 4. Plot metrics vs threshold
plot(thresholds, threshold_results$Accuracy, type = "b",
ylim = c(0, 1), xlab = "Threshold", ylab = "Metric Value",
main = "Metrics vs Threshold")
lines(thresholds, threshold_results$Precision, type = "b", lty = 2)
lines(thresholds, threshold_results$Recall,    type = "b", lty = 3)
lines(thresholds, threshold_results$F1,        type = "b", lty = 4)
legend("topright", legend = c("Accuracy", "Precision", "Recall", "F1"),
lty = 1:4)
# YOUR CODE HERE: Create side-by-side comparison plot
# Separate k grids for regression and classification
k_grid_reg <- 1:30
k_grid_cls <- seq(1, 30, by = 2)
par(mfrow = c(1, 2))
# Panel A: RMSPE vs k (regression)
plot(k_grid_reg, last_mean_rmspe, type = "b",
xlab = "k", ylab = "Mean RMSPE",
main = "(a) Regression: RMSPE vs k")
# Panel B: Accuracy vs k (classification)
plot(k_grid_cls, last_mean_acc, type = "b",
xlab = "k", ylab = "Mean Accuracy",
main = "(b) Classification: Accuracy vs k")
par(mfrow = c(1, 1))
length(last_mean_rmspe)
length(last_mean_acc)
library(tidyverse)
library(caret)
library(knitr)
library(kableExtra)
# YOUR CODE HERE
# 1. Load the data
ames <- read.csv("AmesHousing.csv")
# 2. Subset to the specified variables
ames <- ames %>%
select(SalePrice, Gr.Liv.Area, Total.Bsmt.SF, Garage.Area,
Year.Built, Overall.Qual, Overall.Cond)
# 3. Handle missing values (median imputation on predictors only)
predictors <- c("Gr.Liv.Area", "Total.Bsmt.SF", "Garage.Area",
"Year.Built", "Overall.Qual", "Overall.Cond")
ames <- ames %>%
mutate(across(all_of(predictors), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
# 4. Standardize numeric predictors (NOT SalePrice)
pre_proc <- preProcess(ames[, predictors], method = c("center", "scale"))
ames[, predictors] <- predict(pre_proc, ames[, predictors])
# Sanity check
dim(ames)
sum(is.na(ames))
summary(ames$SalePrice)
# YOUR CODE HERE
set.seed(123)
k_grid <- 1:30
n_iter <- 20
n_boot <- 20
optimal_k  <- numeric(n_iter)
test_RMSPE <- numeric(n_iter)
last_mean_rmspe <- numeric(length(k_grid))
# Save one representative split for Q5
rep_actual <- NULL
rep_preds  <- NULL
# 1. Run 20 iterations, each with a fresh 80/20 split
for (i in 1:n_iter) {
train_idx <- sample(1:nrow(ames), size = 0.8 * nrow(ames))
train <- ames[train_idx, ]
test  <- ames[-train_idx, ]
mean_rmspe <- numeric(length(k_grid))
# 2. Within each iteration: grid search with bootstrap on training set → find optimal k
for (ki in seq_along(k_grid)) {
k <- k_grid[ki]
boot_rmspe <- rep(NA, n_boot)
for (j in 1:n_boot) {
boot_idx   <- sample(1:nrow(train), size = nrow(train), replace = TRUE)
boot_train <- train[boot_idx, ]
boot_val   <- train[-unique(boot_idx), ]
if (nrow(boot_val) == 0) next
model <- knnreg(SalePrice ~ ., data = boot_train, k = k)
preds <- predict(model, boot_val)
boot_rmspe[j] <- sqrt(mean(((preds - boot_val$SalePrice) / boot_val$SalePrice)^2))
}
mean_rmspe[ki] <- mean(boot_rmspe, na.rm = TRUE)
}
# 3. Evaluate with that optimal k on that iteration's test set
optimal_k[i] <- k_grid[which.min(replace(mean_rmspe, is.na(mean_rmspe), Inf))]
if (i == n_iter) last_mean_rmspe <- mean_rmspe
final_model <- knnreg(SalePrice ~ ., data = train, k = optimal_k[i])
test_preds  <- predict(final_model, test)
# 4. Store optimal_k and test_RMSPE for each iteration
test_RMSPE[i] <- sqrt(mean(((test_preds - test$SalePrice) / test$SalePrice)^2))
# Save last iteration as representative split for Q5
if (i == n_iter) {
rep_actual <- test$SalePrice
rep_preds  <- test_preds
}
}
# 5. Report mean RMSPE and SD
cat("Optimal k distribution:\n"); print(table(optimal_k))
cat("\nMean test RMSPE:", round(mean(test_RMSPE), 4))
cat("\nSD test RMSPE:  ", round(sd(test_RMSPE), 4))
# Q3: Mean RMSPE vs k
plot(k_grid, last_mean_rmspe, type = "b",
xlab = "k",
ylab = "Mean RMSPE",
main = "Mean RMSPE vs k (Last Iteration)")
# Q4: Histogram of test RMSPE
hist(test_RMSPE,
main = "Distribution of Test RMSPE (20 Splits)",
xlab = "Test RMSPE",
col = "lightblue",
border = "white")
# Q5: Scatter plot of Actual vs Predicted (representative split)
plot(rep_actual, rep_preds,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Actual vs Predicted Sale Price (Representative Split)",
col  = "steelblue",
pch  = 16)
abline(0, 1, col = "red", lwd = 2)  # perfect prediction line
# YOUR CODE HERE
# 1. Use caret::train() with 5-fold cross-validation to tune k
# Define custom RMSPE summary function for caret::train()
rmspeSummary <- function(data, lev = NULL, model = NULL) {
rmspe <- sqrt(mean(((data$pred - data$obs) / data$obs)^2))
c(RMSPE = rmspe)
}
set.seed(123)
# Define 5-fold cross-validation
train_control <- trainControl(
method = "cv",
number = 5,
summaryFunction = rmspeSummary
)
# Train kNN model with CV over k = 1:30
caret_model <- train(
SalePrice ~ .,
data      = ames,
method    = "knn",
tuneGrid  = data.frame(k = 1:30),
trControl = train_control,
metric    = "RMSPE"
)
# 2. Extract optimal k and corresponding RMSPE
best_k     <- caret_model$bestTune$k
best_rmspe <- min(caret_model$results$RMSPE)
cat("Optimal k (caret train):", best_k)
cat("\nBest CV RMSPE:", round(best_rmspe, 4))
# 3. Compare with manual bootstrap results
cat("\n\nManual bootstrap:")
cat("\nOptimal k distribution:\n"); print(table(optimal_k))
cat("\nMean test RMSPE:", round(mean(test_RMSPE), 4))
# 4. Plot CV RMSPE vs k
plot(caret_model$results$k,
caret_model$results$RMSPE,
type = "b",
xlab = "k",
ylab = "5-fold CV RMSPE",
main = "caret 5-fold CV: RMSPE vs k")
# YOUR CODE HERE
# 1. Load the data
# churn_data <- read.csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
# 2. Remove customerID
# 3. Convert TotalCharges to numeric, handle NAs
# 4. Subset to specified variables
# 5. Convert Churn to factor
# 6. Standardize numeric predictors
# 1. Load the data
churn_data <- read.csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
# 2. Remove customerID
churn_data <- churn_data %>% select(-customerID)
# 3. Convert TotalCharges to numeric (blanks become NA), handle NAs
churn_data$TotalCharges <- as.numeric(churn_data$TotalCharges)
churn_data <- churn_data %>% drop_na()
# 4. Subset to specified variables
churn_data <- churn_data %>%
select(Churn, tenure, MonthlyCharges, TotalCharges,
Contract, InternetService, PaymentMethod)
# 5. Convert Churn to factor
churn_data$Churn <- factor(churn_data$Churn, levels = c("No", "Yes"))
# 6. Standardize numeric predictors
num_predictors <- c("tenure", "MonthlyCharges", "TotalCharges")
pre_proc2 <- preProcess(churn_data[, num_predictors], method = c("center", "scale"))
churn_data[, num_predictors] <- predict(pre_proc2, churn_data[, num_predictors])
# Ensure categoricals are factors
churn_data$Contract        <- factor(churn_data$Contract)
churn_data$InternetService <- factor(churn_data$InternetService)
churn_data$PaymentMethod   <- factor(churn_data$PaymentMethod)
# Sanity check
dim(churn_data)
sum(is.na(churn_data))
table(churn_data$Churn)
prop.table(table(churn_data$Churn))
# YOUR CODE HERE
# 1. Run 20 iterations, each with a fresh stratified 80/20 split
# 2. Within each iteration: grid search with bootstrap on training set → find optimal k
# 3. Evaluate with that optimal k on that iteration's test set
# 4. Store optimal_k and all metrics for each iteration
# 5. Report mean and SD for all metrics
set.seed(123)
k_grid <- seq(1, 30, by = 2)  # odd values only
n_iter <- 20
n_boot <- 20
optimal_k_cls <- numeric(n_iter)
acc_vec       <- numeric(n_iter)
prec_vec      <- numeric(n_iter)
rec_vec       <- numeric(n_iter)
f1_vec        <- numeric(n_iter)
last_mean_acc <- numeric(length(k_grid))
# Save representative split for Q5
rep_cm <- NULL
# 1. Run 20 iterations, each with a fresh stratified 80/20 split
for (i in 1:n_iter) {
# Different seed each iteration for different stratified splits
set.seed(123 + i)
train_idx <- createDataPartition(churn_data$Churn, p = 0.8, list = FALSE)
train_raw <- churn_data[train_idx, ]
test_raw  <- churn_data[-train_idx, ]
# FIX: dummyVars on predictors only (exclude Churn)
dummies   <- dummyVars(~ ., data = train_raw %>% select(-Churn))
train_mat <- predict(dummies, train_raw %>% select(-Churn))  # changed
test_mat  <- predict(dummies, test_raw  %>% select(-Churn))  # changed
# Standardize after dummy encoding: fit on train only, apply to both
pre_proc_inner <- preProcess(train_mat, method = c("center", "scale"))
train_mat      <- predict(pre_proc_inner, train_mat)
test_mat       <- predict(pre_proc_inner, test_mat)
# Re-factor Churn explicitly
train <- data.frame(train_mat, Churn = factor(train_raw$Churn, levels = c("No", "Yes")))
test  <- data.frame(test_mat,  Churn = factor(test_raw$Churn,  levels = c("No", "Yes")))
mean_acc <- numeric(length(k_grid))
# 2. Within each iteration: grid search with bootstrap on training set → find optimal k
for (ki in seq_along(k_grid)) {
k <- k_grid[ki]
boot_acc <- rep(NA, n_boot)
for (j in 1:n_boot) {
boot_idx   <- sample(1:nrow(train), size = nrow(train), replace = TRUE)
boot_train <- train[boot_idx, ]
boot_val   <- train[-unique(boot_idx), ]
if (nrow(boot_val) == 0) next
model <- knn3(Churn ~ ., data = boot_train, k = k)
preds <- predict(model, boot_val, type = "class")
boot_acc[j] <- mean(preds == boot_val$Churn)
}
mean_acc[ki] <- mean(boot_acc, na.rm = TRUE)
}
# 3. Evaluate with that optimal k on that iteration's test set
optimal_k_cls[i] <- k_grid[which.max(replace(mean_acc, is.na(mean_acc), -Inf))]
if (i == n_iter) last_mean_acc <- mean_acc
final_model <- knn3(Churn ~ ., data = train, k = optimal_k_cls[i])
test_preds  <- predict(final_model, test, type = "class")
# 4. Store optimal_k and all metrics for each iteration
cm          <- confusionMatrix(test_preds, test$Churn, positive = "Yes")
acc_vec[i]  <- cm$overall["Accuracy"]
prec_vec[i] <- cm$byClass["Precision"]
rec_vec[i]  <- cm$byClass["Recall"]
f1_vec[i]   <- cm$byClass["F1"]
# Save last iteration for Q5
if (i == n_iter) rep_cm <- cm
}
# 5. Report mean and SD for all metrics
metrics_df <- data.frame(
Metric = c("Accuracy", "Precision", "Recall", "F1"),
Mean   = round(c(mean(acc_vec), mean(prec_vec), mean(rec_vec), mean(f1_vec)), 4),
SD     = round(c(sd(acc_vec),   sd(prec_vec),   sd(rec_vec),   sd(f1_vec)),   4)
)
print(metrics_df)
cat("\nOptimal k distribution:\n"); print(table(optimal_k_cls))
# Q3: Mean accuracy vs k
plot(k_grid, last_mean_acc, type = "b",
xlab = "k",
ylab = "Mean Accuracy",
main = "Mean Accuracy vs k (Last Iteration)")
# Q4: Histograms for all metrics
par(mfrow = c(2, 2))
hist(acc_vec,  main = "Accuracy",  xlab = "Accuracy",  col = "lightblue", border = "white")
hist(prec_vec, main = "Precision", xlab = "Precision", col = "lightblue", border = "white")
hist(rec_vec,  main = "Recall",    xlab = "Recall",    col = "lightblue", border = "white")
hist(f1_vec,   main = "F1-Score",  xlab = "F1",        col = "lightblue", border = "white")
par(mfrow = c(1, 1))
# Q5: Confusion matrix from representative split
print(rep_cm)
# YOUR CODE HERE: Display confusion matrix from one representative split
print(rep_cm)
# YOUR CODE HERE (optional)
# 1. Use knn3 with type = "prob" to get predicted probabilities
# 2. Try multiple thresholds
# 3. Compute metrics for each threshold
# 4. Create a table or plot
# YOUR CODE HERE (optional)
# 1. Use knn3 with type = "prob" to get predicted probabilities
# 2. Try multiple thresholds
# 3. Compute metrics for each threshold
# 4. Create a table or plot
# Use optimal k from last iteration and representative split
set.seed(123 + 20)
train_idx_rep <- createDataPartition(churn_data$Churn, p = 0.8, list = FALSE)
train_raw_rep <- churn_data[train_idx_rep, ]
test_raw_rep  <- churn_data[-train_idx_rep, ]
# dummyVars on predictors only
dummies_rep   <- dummyVars(~ ., data = train_raw_rep %>% select(-Churn))
# FIX: predict on predictors only (exclude Churn)
train_mat_rep <- predict(dummies_rep, train_raw_rep %>% select(-Churn))
test_mat_rep  <- predict(dummies_rep, test_raw_rep  %>% select(-Churn))
# Standardize: fit on train only, apply to both
pre_proc_rep  <- preProcess(train_mat_rep, method = c("center", "scale"))
train_mat_rep <- predict(pre_proc_rep, train_mat_rep)
test_mat_rep  <- predict(pre_proc_rep, test_mat_rep)
train_rep <- data.frame(train_mat_rep, Churn = factor(train_raw_rep$Churn, levels = c("No", "Yes")))
test_rep  <- data.frame(test_mat_rep,  Churn = factor(test_raw_rep$Churn,  levels = c("No", "Yes")))
# Fit with optimal k from last iteration
best_k_cls  <- optimal_k_cls[20]
bonus_model <- knn3(Churn ~ ., data = train_rep, k = best_k_cls)
# 1. Get predicted probabilities
probs <- predict(bonus_model, test_rep, type = "prob")
# 2. Try multiple thresholds
thresholds <- c(0.2, 0.3, 0.4, 0.5, 0.6)
threshold_results <- data.frame(
Threshold = thresholds,
Accuracy  = NA, Precision = NA, Recall = NA, F1 = NA
)
# 3. Compute metrics for each threshold
for (t in seq_along(thresholds)) {
pred_custom <- ifelse(probs[, "Yes"] >= thresholds[t], "Yes", "No")
pred_custom <- factor(pred_custom, levels = c("No", "Yes"))
cm_t <- confusionMatrix(pred_custom, test_rep$Churn, positive = "Yes")
threshold_results$Accuracy[t]  <- round(cm_t$overall["Accuracy"], 4)
threshold_results$Precision[t] <- round(cm_t$byClass["Precision"], 4)
threshold_results$Recall[t]    <- round(cm_t$byClass["Recall"], 4)
threshold_results$F1[t]        <- round(cm_t$byClass["F1"], 4)
}
print(threshold_results)
# 4. Plot metrics vs threshold
plot(thresholds, threshold_results$Accuracy, type = "b",
ylim = c(0, 1), xlab = "Threshold", ylab = "Metric Value",
main = "Metrics vs Threshold")
lines(thresholds, threshold_results$Precision, type = "b", lty = 2)
lines(thresholds, threshold_results$Recall,    type = "b", lty = 3)
lines(thresholds, threshold_results$F1,        type = "b", lty = 4)
legend("topright", legend = c("Accuracy", "Precision", "Recall", "F1"),
lty = 1:4)
# YOUR CODE HERE: Create side-by-side comparison plot
# Separate k grids for regression and classification
k_grid_reg <- 1:30
k_grid_cls <- seq(1, 30, by = 2)
par(mfrow = c(1, 2))
# Panel A: RMSPE vs k (regression)
plot(k_grid_reg, last_mean_rmspe, type = "b",
xlab = "k", ylab = "Mean RMSPE",
main = "(a) Regression: RMSPE vs k")
# Panel B: Accuracy vs k (classification)
plot(k_grid_cls, last_mean_acc, type = "b",
xlab = "k", ylab = "Mean Accuracy",
main = "(b) Classification: Accuracy vs k")
par(mfrow = c(1, 1))
length(last_mean_rmspe)
length(last_mean_acc)
# YOUR CODE HERE (optional)
# 1. Use knn3 with type = "prob" to get predicted probabilities
# 2. Try multiple thresholds
# 3. Compute metrics for each threshold
# 4. Create a table or plot
# YOUR CODE HERE (optional)
# 1. Use knn3 with type = "prob" to get predicted probabilities
# 2. Try multiple thresholds
# 3. Compute metrics for each threshold
# 4. Create a table or plot
# Use optimal k from last iteration and representative split
set.seed(123 + 20)
train_idx_rep <- createDataPartition(churn_data$Churn, p = 0.8, list = FALSE)
train_raw_rep <- churn_data[train_idx_rep, ]
test_raw_rep  <- churn_data[-train_idx_rep, ]
# dummyVars on predictors only
dummies_rep   <- dummyVars(~ ., data = train_raw_rep %>% select(-Churn))
# FIX: predict on predictors only (exclude Churn)
train_mat_rep <- predict(dummies_rep, train_raw_rep %>% select(-Churn))
test_mat_rep  <- predict(dummies_rep, test_raw_rep  %>% select(-Churn))
# Standardize: fit on train only, apply to both
pre_proc_rep  <- preProcess(train_mat_rep, method = c("center", "scale"))
train_mat_rep <- predict(pre_proc_rep, train_mat_rep)
test_mat_rep  <- predict(pre_proc_rep, test_mat_rep)
train_rep <- data.frame(train_mat_rep, Churn = factor(train_raw_rep$Churn, levels = c("No", "Yes")))
test_rep  <- data.frame(test_mat_rep,  Churn = factor(test_raw_rep$Churn,  levels = c("No", "Yes")))
# Fit with optimal k from last iteration
best_k_cls  <- optimal_k_cls[20]
bonus_model <- knn3(Churn ~ ., data = train_rep, k = best_k_cls)
# 1. Get predicted probabilities
probs <- predict(bonus_model, test_rep, type = "prob")
# 2. Try multiple thresholds
thresholds <- c(0.2, 0.3, 0.4, 0.5, 0.6)
threshold_results <- data.frame(
Threshold = thresholds,
Accuracy  = NA, Precision = NA, Recall = NA, F1 = NA
)
# 3. Compute metrics for each threshold
for (t in seq_along(thresholds)) {
pred_custom <- ifelse(probs[, "Yes"] >= thresholds[t], "Yes", "No")
pred_custom <- factor(pred_custom, levels = c("No", "Yes"))
cm_t <- confusionMatrix(pred_custom, test_rep$Churn, positive = "Yes")
threshold_results$Accuracy[t]  <- round(cm_t$overall["Accuracy"], 4)
threshold_results$Precision[t] <- round(cm_t$byClass["Precision"], 4)
threshold_results$Recall[t]    <- round(cm_t$byClass["Recall"], 4)
threshold_results$F1[t]        <- round(cm_t$byClass["F1"], 4)
}
kable(threshold_results, digits = 4, caption = "Metrics by Threshold")
# 4. Plot metrics vs threshold
plot(thresholds, threshold_results$Accuracy, type = "b",
ylim = c(0, 1), xlab = "Threshold", ylab = "Metric Value",
main = "Metrics vs Threshold")
lines(thresholds, threshold_results$Precision, type = "b", lty = 2)
lines(thresholds, threshold_results$Recall,    type = "b", lty = 3)
lines(thresholds, threshold_results$F1,        type = "b", lty = 4)
legend("topright", legend = c("Accuracy", "Precision", "Recall", "F1"),
lty = 1:4)
# YOUR CODE HERE: Create side-by-side comparison plot
# Separate k grids for regression and classification
k_grid_reg <- 1:30
k_grid_cls <- seq(1, 30, by = 2)
par(mfrow = c(1, 2))
# Panel A: RMSPE vs k (regression)
plot(k_grid_reg, last_mean_rmspe, type = "b",
xlab = "k", ylab = "Mean RMSPE",
main = "(a) Regression: RMSPE vs k")
# Panel B: Accuracy vs k (classification)
plot(k_grid_cls, last_mean_acc, type = "b",
xlab = "k", ylab = "Mean Accuracy",
main = "(b) Classification: Accuracy vs k")
par(mfrow = c(1, 1))
